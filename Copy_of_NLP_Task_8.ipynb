{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP_Task_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3UhYodTdrnfndZ8Jj4dlB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayabharathiRavi/D19_NLP_Task_8/blob/main/Copy_of_NLP_Task_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YQ0vnQ9dzn8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Given dataset"
      ],
      "metadata": {
        "id": "ueHQB0IdLTpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = open(\"/content/English.csv\")\n",
        "text = text_file.read()\n",
        "print (type(text))\n",
        "print (len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxEyXk4OA6c1",
        "outputId": "2545aa83-1fea-431b-ce21-a9d65a0959fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "1048567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing package"
      ],
      "metadata": {
        "id": "BRO1TZIGLZYz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BshTLAlnzjoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c03c15-d104-40b8-9d10-add1e931275c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlbXFBbIGuSG",
        "outputId": "706c30e0-3413-4bde-e83a-3d4d206df0fb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: translate in /usr/local/lib/python3.7/dist-packages (3.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from translate) (7.1.2)\n",
            "Requirement already satisfied: libretranslatepy==2.1.1 in /usr/local/lib/python3.7/dist-packages (from translate) (2.1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from translate) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from translate) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task-1**\n",
        "\tYou need to translate each word or sentence from English to Spanish, French and German \n"
      ],
      "metadata": {
        "id": "3Stq-KtZHQFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cleaning the dataset**\n",
        "\n",
        "\n",
        "*   sentence tokenization\n",
        "*   word tokenization\n",
        "\n"
      ],
      "metadata": {
        "id": "1QwBArsSLw5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(text)"
      ],
      "metadata": {
        "id": "9XPkzUWtLmiD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=[]\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for sentence in sentences:\n",
        "    s.append(sentence)"
      ],
      "metadata": {
        "id": "SXp12-DhNJQg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(s[:10],len(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSD8x-plMv4z",
        "outputId": "f1f98198-2195-4dc4-fb96-62f62b403dd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['English words/sentences\\nHi.', 'Run!', 'Run!', 'Who?', 'Wow!', 'Fire!', 'Help!', 'Jump.', 'Stop!', 'Stop!'] 53138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=[]\n",
        "for sentence in sentences: # lopoping through every sentence\n",
        "    words = nltk.word_tokenize(sentence) # we are extracting the important words in the sentence\n",
        "    w.append(words)"
      ],
      "metadata": {
        "id": "eUcweOunAh36"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w[:5],len(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xF_TXH0OLVm",
        "outputId": "e0efc0e7-b2c7-4fa5-d069-c9bb015655f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['English', 'words/sentences', 'Hi', '.'], ['Run', '!'], ['Run', '!'], ['Who', '?'], ['Wow', '!']] 53138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**stop words removal**"
      ],
      "metadata": {
        "id": "RywTljewSmkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "os.listdir('/root/nltk_data/corpora/stopwords/')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stop_words = list(stop_words)"
      ],
      "metadata": {
        "id": "kyY4wa0YSqWs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punctuation removal**"
      ],
      "metadata": {
        "id": "sToxpmZnQEHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "p=[]\n",
        "for i in s:\n",
        "  i=i.lower()\n",
        "  P=tokenizer.tokenize(i)\n",
        "  p.extend((P))"
      ],
      "metadata": {
        "id": "w1thNMijQCl0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1=list(set(p))\n",
        "len(s),len(p),p[:2],len(p1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMes5eebSbfV",
        "outputId": "b15d6016-4c29-45b6-a5ca-7b632672d93a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53138, 223592, ['english', 'words'], 6091)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_punc=[]\n",
        "for i in p1:\n",
        "  if i not in stop_words:\n",
        "    no_punc.append(i)\n",
        "print(no_punc[:5],len(no_punc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNp0XbqeQkpT",
        "outputId": "0a670c1a-2185-4a84-ff44-98d35b08cd89"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['obsolete', 'trusts', 'knits', 'differ', 'baffled'] 5948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "p0ebl4lpOVk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "lemma=[]\n",
        "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
        "     l=lemmatizer.lemmatize(word, pos)\n",
        "     lemma.append(l)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "for i in no_punc:\n",
        "  compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = i, pos = wordnet.VERB)"
      ],
      "metadata": {
        "id": "RGjRFElVpHnt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma[:10],len(lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KVvBQbmObhk",
        "outputId": "13882473-d314-4294-c108-9d24b9962f1b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['obsolete',\n",
              "  'trust',\n",
              "  'knit',\n",
              "  'differ',\n",
              "  'baffle',\n",
              "  'stake',\n",
              "  'note',\n",
              "  'discover',\n",
              "  'learn',\n",
              "  'househusband'],\n",
              " 5948)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No punctuation sentences**"
      ],
      "metadata": {
        "id": "YzxSdorSrj6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sentence_no_punc = []\n",
        "\n",
        "for w in set(s):\n",
        "  text = w\n",
        "  text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
        "  sentence_no_punc.append(text.lower())\n",
        "\n",
        "print (sentence_no_punc[:10])\n",
        "print (len(sentence_no_punc))"
      ],
      "metadata": {
        "id": "k1RjElVgPN9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eccabf82-bae0-4c18-a67c-4faa9fd775da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the door will not open', 'it was so obvious', 'go ahead', 'dont litter', 'where is it located', 'they dont eat meat', 'toms my friend', 'i dont like oysters', 'youre not helping', 'i know who it was']\n",
            "34063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(sentence_no_punc)),len(sentence_no_punc)"
      ],
      "metadata": {
        "id": "LMTOfEtMlgJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c504fbf9-66b6-4b6a-e439-dc215d663e0d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33943, 34063)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Oz8HZfgRHfLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**English to French**"
      ],
      "metadata": {
        "id": "lJWDONdrHY5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S=[set(sentence_no_punc)]\n",
        "s=[]\n",
        "for i in S:\n",
        "  for j in i:\n",
        "    s.append(j)"
      ],
      "metadata": {
        "id": "2CjlCmE12vUK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from translate import Translator\n",
        "fr=[]\n",
        "for i in s[:50]:\n",
        "  w=i\n",
        "  translator= Translator(to_lang=\"fr\")\n",
        "  translation = translator.translate(w)\n",
        "  fr.append(translation)\n",
        "#fr..french"
      ],
      "metadata": {
        "id": "5oXxNzQ-lgIb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in zip(s[:10],fr[:10]):\n",
        "  print(f\"English sentence:{x}\")\n",
        "  print(f\"French sentence:{y}\")  \n",
        "  print() \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "753gFGdi4NDc",
        "outputId": "d2acdb45-8fb0-4f7f-9daa-6944a46f9e56"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "english sentence:this is your fault\n",
            "french sentence:C'est de ta faute.\n",
            "\n",
            "english sentence:tom isnt reliable\n",
            "french sentence:tom n’est pas fiable\n",
            "\n",
            "english sentence:is tom on this ship\n",
            "french sentence:est tom sur ce navire\n",
            "\n",
            "english sentence:the children are safe\n",
            "french sentence:Les enfants sont en sécurité.\n",
            "\n",
            "english sentence:toms married\n",
            "french sentence:toms mariés\n",
            "\n",
            "english sentence:i can hardly breathe\n",
            "french sentence:Je n'ai pas peur de prendre position\n",
            "\n",
            "english sentence:its extremely unfair\n",
            "french sentence:c’est extrêmement injuste\n",
            "\n",
            "english sentence:i always lose\n",
            "french sentence:Je perds toujours\n",
            "\n",
            "english sentence:he pretends to be deaf\n",
            "french sentence:Il prétend être sourd.\n",
            "\n",
            "english sentence:i dont eat pork\n",
            "french sentence:Je ne mange pas de porc\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**English to Spanish**"
      ],
      "metadata": {
        "id": "PG-bTehl6Hgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from translate import Translator\n",
        "es=[]\n",
        "for i in s[:50]:\n",
        "  translator= Translator(to_lang=\"es\")\n",
        "  w=i\n",
        "  translation = translator.translate(w)\n",
        "  es.append(translation)\n",
        "#spanish..es"
      ],
      "metadata": {
        "id": "iFbG2RDmlgDQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in zip(s[:10],es[:10]):\n",
        "  print(f\"English sentence:{x}\")\n",
        "  print(f\"Spanish sentence:{y}\")  \n",
        "  print() \n"
      ],
      "metadata": {
        "id": "SSP-E6KBlgCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a220d4-22ae-4c6d-b167-1f71904792e4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence:this is your fault\n",
            "Spanish sentence:Es todo culpa tuya.\n",
            "\n",
            "English sentence:tom isnt reliable\n",
            "Spanish sentence:tom no es fiable\n",
            "\n",
            "English sentence:is tom on this ship\n",
            "Spanish sentence:es tom en este barco\n",
            "\n",
            "English sentence:the children are safe\n",
            "Spanish sentence:Los hijos son:\n",
            "\n",
            "English sentence:toms married\n",
            "Spanish sentence:toms casados\n",
            "\n",
            "English sentence:i can hardly breathe\n",
            "Spanish sentence:Apenas puedo respirar.\n",
            "\n",
            "English sentence:its extremely unfair\n",
            "Spanish sentence:es extremadamente injusto\n",
            "\n",
            "English sentence:i always lose\n",
            "Spanish sentence:Siempre\n",
            "\n",
            "English sentence:he pretends to be deaf\n",
            "Spanish sentence:Él finge ser sordo.\n",
            "\n",
            "English sentence:i dont eat pork\n",
            "Spanish sentence:No como cerdo\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**English to German**"
      ],
      "metadata": {
        "id": "R6HwCei_5_df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from translate import Translator\n",
        "de=[]\n",
        "for i in s[:50]:\n",
        "  translator= Translator(to_lang=\"de\")\n",
        "  w=i\n",
        "  translation = translator.translate(w)\n",
        "  de.append(translation)\n",
        "#german..de"
      ],
      "metadata": {
        "id": "TyWiNSkiGdgU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in zip(s[:10],de[:10]):\n",
        "  print(f\"English sentence:{x}\")\n",
        "  print(f\"German sentence:{y}\")  \n",
        "  print() \n"
      ],
      "metadata": {
        "id": "_aCuXzmkGda2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86db7b9c-0082-44d5-8b5e-d78cd01515d8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence:this is your fault\n",
            "German sentence:Das ist deine Schuld.\n",
            "\n",
            "English sentence:tom isnt reliable\n",
            "German sentence:Tom ist nicht zuverlässig\n",
            "\n",
            "English sentence:is tom on this ship\n",
            "German sentence:Ist Tom auf diesem Schiff\n",
            "\n",
            "English sentence:the children are safe\n",
            "German sentence:Die Kinder sind in Sicherheit.\n",
            "\n",
            "English sentence:toms married\n",
            "German sentence:Toms heiratete\n",
            "\n",
            "English sentence:i can hardly breathe\n",
            "German sentence:...doch kann ich nicht!\n",
            "\n",
            "English sentence:its extremely unfair\n",
            "German sentence:es ist extrem unfair\n",
            "\n",
            "English sentence:i always lose\n",
            "German sentence:ich empfehle sie\n",
            "\n",
            "English sentence:he pretends to be deaf\n",
            "German sentence:Er stellt sich taub.\n",
            "\n",
            "English sentence:i dont eat pork\n",
            "German sentence:Ich esse kein Schweinefleisch\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-2\n",
        "\tCreate a program that needs to automatically correct that spelling from the word or a given sentence. (Language : English) \n",
        "Note:  show 100 data point outputs both in words and sentence.\n"
      ],
      "metadata": {
        "id": "nrLnzJHE6VPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "words=[]\n",
        "def compare_stemmer_and_lemmatizer(lemmatizer, word, pos):\n",
        "     l=lemmatizer.lemmatize(word, pos)\n",
        "     words.append(l)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for i in no_punc:\n",
        "  compare_stemmer_and_lemmatizer(lemmatizer, word = i, pos = wordnet.VERB)"
      ],
      "metadata": {
        "id": "pDzggLhWGdaL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words[:101]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd5DfoFrxwct",
        "outputId": "e45d7a8a-f170-4946-9591-7bd65224b8e1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['obsolete',\n",
              " 'trust',\n",
              " 'knit',\n",
              " 'differ',\n",
              " 'baffle',\n",
              " 'stake',\n",
              " 'note',\n",
              " 'discover',\n",
              " 'learn',\n",
              " 'househusband',\n",
              " 'keyboard',\n",
              " 'crush',\n",
              " 'sin',\n",
              " 'talk',\n",
              " 'miserable',\n",
              " 'single',\n",
              " 'permanent',\n",
              " 'shift',\n",
              " 'comfort',\n",
              " 'bare',\n",
              " 'vegetarians',\n",
              " 'get',\n",
              " 'step',\n",
              " 'british',\n",
              " 'compare',\n",
              " 'sweat',\n",
              " 'investigate',\n",
              " 'lighten',\n",
              " 'acquire',\n",
              " 'movie',\n",
              " 'ideal',\n",
              " 'pilgrimage',\n",
              " 'shortly',\n",
              " 'flatter',\n",
              " 'laugh',\n",
              " 'two',\n",
              " 'genuine',\n",
              " 'stutter',\n",
              " 'sample',\n",
              " 'lead',\n",
              " 'chest',\n",
              " 'let',\n",
              " 'nicely',\n",
              " 'boss',\n",
              " 'request',\n",
              " 'convince',\n",
              " 'uncommon',\n",
              " 'drown',\n",
              " 'quiet',\n",
              " 'brush',\n",
              " 'strange',\n",
              " 'lick',\n",
              " 'acrylic',\n",
              " 'hide',\n",
              " 'comfort',\n",
              " 'banana',\n",
              " 'meet',\n",
              " 'ninety',\n",
              " 'internet',\n",
              " 'attendance',\n",
              " 'claustrophobic',\n",
              " 'personal',\n",
              " 'spring',\n",
              " 'afternoon',\n",
              " 'gain',\n",
              " 'sore',\n",
              " 'protest',\n",
              " 'beg',\n",
              " 'simple',\n",
              " 'use',\n",
              " 'ipad',\n",
              " 'briefly',\n",
              " 'listen',\n",
              " 'dublin',\n",
              " 'onions',\n",
              " 'crisis',\n",
              " 'argue',\n",
              " 'jello',\n",
              " 'sandwich',\n",
              " 'moron',\n",
              " 'scam',\n",
              " 'undecided',\n",
              " 'rent',\n",
              " 'impress',\n",
              " 'horrify',\n",
              " 'voice',\n",
              " 'deer',\n",
              " 'fashionable',\n",
              " 'objection',\n",
              " 'net',\n",
              " 'irresponsible',\n",
              " 'color',\n",
              " 'tattoo',\n",
              " 'exception',\n",
              " 'sec',\n",
              " 'shirt',\n",
              " 'adopt',\n",
              " 'doubt',\n",
              " 'wisdom',\n",
              " 'boat',\n",
              " 'blog']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task-3\n",
        "\tCreate an application that should be used by the HR Team to filter the resume based on the Skills.\n"
      ],
      "metadata": {
        "id": "ePqFid2EvM65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pbPLvoAgGdUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task-4\n",
        "\tCreate a chatbot for Hotel Management to Book Rooms \n",
        "\n",
        "Details collected from : Customer Name, Mobile Number, Address, ID proof, and Room Type and date of arrival and departure date. Keep some eligibility to Book the Room .\n",
        "All through voice or text \n",
        "Note: You can use Rule Based Chatbot or NLP Based Chatbot \n"
      ],
      "metadata": {
        "id": "MvDzrGo_vVOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7z6JkKhYGdUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BdqDSl_yj5-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}